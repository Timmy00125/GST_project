# Install required libraries
!pip install tensorflow pandas numpy scikit-learn imbalanced-learn

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import RobustScaler, MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras import regularizers
import gc




# Load datasets
patients = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI_ML/Population_health/dataset/patients.csv')
conditions = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI_ML/Population_health/dataset/conditions.csv')
observations = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI_ML/Population_health/dataset/observations.csv')


# ... (keep previous imports and data loading)

# Preprocess data with vital signs
# ---------------------------------------------------------------
# Extract age with clipping
patients['age'] = (pd.to_datetime('today') - pd.to_datetime(patients['BIRTHDATE'])).dt.days // 365
patients['age'] = np.clip(patients['age'], 18, 100)  # Assume adult patients

# Define vital signs mapping
vital_signs = {
    'Body Mass Index': 'bmi',
    'Body Weight': 'weight',
    'Systolic Blood Pressure': 'sys_bp',
    'Diastolic Blood Pressure': 'dia_bp',
    'Heart rate': 'heart_rate',
    'Body Height': 'height'
}

# Process vital signs from observations
vitals_df = observations[observations['DESCRIPTION'].isin(vital_signs.keys())]
vitals_df = vitals_df.pivot_table(index='PATIENT', 
                                columns='DESCRIPTION', 
                                values='VALUE',
                                aggfunc='first').reset_index()

# Rename columns using vital signs mapping
vitals_df = vitals_df.rename(columns=vital_signs)

# Clean and convert numeric values
for col in vital_signs.values():
    vitals_df[col] = pd.to_numeric(vitals_df[col], errors='coerce')


    
# Handle outliers with medical constraints
vitals_df['sys_bp'] = np.clip(vitals_df['sys_bp'], 80, 200)
vitals_df['dia_bp'] = np.clip(vitals_df['dia_bp'], 40, 120)
vitals_df['heart_rate'] = np.clip(vitals_df['heart_rate'], 30, 200)
vitals_df['height'] = np.clip(vitals_df['height'], 120, 220)  # in cm
vitals_df['weight'] = np.clip(vitals_df['weight'], 30, 200)   # in kg

# Calculate BMI if not present using height/weight
if 'bmi' not in vitals_df.columns:
    vitals_df['bmi'] = vitals_df['weight'] / ((vitals_df['height']/100) ** 2)

# Merge all data
df = pd.merge(patients, vitals_df, left_on='Id', right_on='PATIENT', how='inner')
df = pd.merge(df, conditions, left_on='Id', right_on='PATIENT', how='inner')



# Feature engineering
# ---------------------------------------------------------------
# Blood pressure categories
df['bp_category'] = pd.cut(df['sys_bp'],
                          bins=[0, 120, 130, 140, 180, 300],
                          labels=['normal', 'elevated', 'stage1', 'stage2', 'crisis'])

# BMI categories
df['bmi_class'] = pd.cut(df['bmi'],
                        bins=[0, 18.5, 25, 30, 40, 100],
                        labels=['underweight', 'normal', 'overweight', 'obese', 'severe_obese'])

# Create pulse pressure
df['pulse_pressure'] = df['sys_bp'] - df['dia_bp']

# Age-decade interaction
df['age_decade'] = (df['age'] // 10) * 10

# Convert categorical features
cat_features = pd.get_dummies(df[['GENDER', 'bp_category', 'bmi_class', 'age_decade']],
                             columns=['GENDER', 'bp_category', 'bmi_class', 'age_decade'])

# Final feature set
numeric_features = df[['Id', 'age', 'bmi', 'sys_bp', 'dia_bp', 
                      'heart_rate', 'pulse_pressure']].drop_duplicates()

final_features = pd.concat([numeric_features, cat_features], axis=1)




# Filter and process conditions
# ---------------------------------------------------------------
# Filter to only include common conditions
condition_counts = df['DESCRIPTION'].value_counts()
common_conditions = condition_counts[condition_counts > 50].index.tolist()
df = df[df['DESCRIPTION'].isin(common_conditions)]

patient_conditions = df.groupby('Id')['DESCRIPTION'].apply(list).reset_index()

# Multi-label binarizer
mlb = MultiLabelBinarizer(classes=common_conditions)
labels = mlb.fit_transform(patient_conditions['DESCRIPTION'])

# Align features and labels
merged_df = pd.merge(final_features, patient_conditions, on='Id', how='inner')
features = merged_df.drop(['Id', 'DESCRIPTION'], axis=1).values
labels = mlb.transform(merged_df['DESCRIPTION'])


# Handle missing values
features = np.nan_to_num(features)



# Split data with stratification
X_train, X_test, y_train, y_test = train_test_split(
    features, labels, 
    test_size=0.2, 
    random_state=42,
    stratify=labels.sum(axis=1)
)


# Explicit type conversion (safety check)
X_train = X_train.astype(np.float32)
y_train = y_train.astype(np.float32)
X_test = X_test.astype(np.float32)
y_test = y_test.astype(np.float32)


# Enhanced Model Architecture
# ---------------------------------------------------------------
model = tf.keras.Sequential([
    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],),
                         kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_regularizer=tf.keras.regularizers.l2(0.005)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(common_conditions), activation='sigmoid')
])

# Custom optimizer with weight decay
# Removed ExponentialDecay from the Adam optimizer initialization
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) 

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=[
        tf.keras.metrics.AUC(name='auc', multi_label=True),
        tf.keras.metrics.PrecisionAtRecall(0.5, name='precision'),
        tf.keras.metrics.RecallAtPrecision(0.5, name='recall')
    ]
)


from sklearn.utils.class_weight import compute_class_weight

class_weights = []
for i in range(labels.shape[1]):
    # Convert classes to numpy array and ensure binary
    class_weights.append(
        compute_class_weight('balanced',
                            classes=np.array([0, 1]),  # <-- Fix here
                            y=labels[:, i])[1]
    )
    
class_weights = {i: w for i, w in enumerate(class_weights)}



# Check for NaNs/Infs in features
print("Feature NaNs:", np.isnan(X_train).sum())
print("Feature Infs:", np.isinf(X_train).sum())

# Check labels
print("Label NaNs:", np.isnan(y_train).sum())
print("Label Infs:", np.isinf(y_train).sum())

# Add small epsilon to zeros if needed
X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0) + 1e-8




history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=128,
    validation_split=0.2,
    class_weight=class_weights,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=3)
    ]
)
